# @package _global_

hydra:
  run:
    dir: ${trainer.default_root_dir}
  sweep:
    dir: ${trainer.default_root_dir}
    subdir: ${hydra.job.override_dirname}

  job_logging:
    version: 1
    formatters:
      default:
        format: "[%(asctime)s][%(levelname)s] %(name)s %(lineno)d: %(message)s"
      colorful:
        '()': 'colorlog.ColoredFormatter'
        format: "[%(yellow)s%(asctime)s%(reset)s][%(log_color)s%(levelname)s%(reset)s][%(blue)s%(name)s L%(lineno)d%(reset)s]: %(message)s"
        datefmt: "%Y-%m-%d %H:%M:%S"
        log_colors:
          DEBUG: purple
          INFO: green
          WARNING: yellow
          ERROR: red
          CRITICAL: red
    handlers:
      console:
        level: INFO
        class: logging.StreamHandler
        formatter: colorful
      file:
        level: DEBUG
        formatter: default
        filename: "${hydra.runtime.output_dir}/${hydra.job.name}.log"
    root:
      level: INFO
      handlers: [ file, console ]
  verbose: [ "__main__", "cocap" ]

model:
  lr: 1e-4
  clip_lr: 1e-6
  warmup_ratio: 0.05
  lr_decay_gamma: 0.95

trainer:
  # default_root_dir: # Required
  strategy: "ddp_find_unused_parameters_true"
  max_epochs: 20
  accumulate_grad_batches: 4
  logger:
    _target_: "pytorch_lightning.loggers.tensorboard.TensorBoardLogger"
    save_dir: ${trainer.default_root_dir}
  log_every_n_steps: 1
  num_sanity_val_steps: -1
  callbacks:
    - _target_: "pytorch_lightning.callbacks.LearningRateMonitor"
      logging_interval: "step"
    - _target_: "pytorch_lightning.callbacks.ModelCheckpoint"
      save_top_k: -1

train_dataloader:
  dataset:
    split: "train"
  num_workers: 4
  batch_size: 2
  prefetch_factor: 4
  shuffle: True
  pin_memory: True
  multiprocessing_context: "fork"

val_dataloader:
  dataset:
    split: "test"
  num_workers: 4
  batch_size: 2
  prefetch_factor: 4
  shuffle: False
  pin_memory: True
  multiprocessing_context: "fork"
